{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nombre: **06_modelo_genero**\n",
    "#### Autores: _Sergio Cañón  Laiz y Ignacio Ruiz de Zuazu Echavarría_\n",
    "#### CUNEF\n",
    "#### 15/01/2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerías \n",
    "\n",
    "import numpy as np  # álgebra lineal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import glob  # pathnames\n",
    "import cv2  # lectura y procesamiento de las imagenes\n",
    "\n",
    "import matplotlib.pyplot as plt  # gráficos\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import os  # lectura de directorios\n",
    "\n",
    "import keras \n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import time #tiempos de ejecución\n",
    "from random import shuffle\n",
    "from keras.preprocessing import image\n",
    "import imageio\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longuitud del los datos son de: 24099\n"
     ]
    }
   ],
   "source": [
    "categorical_labels_gender = np.load(r'..\\data\\04_data_array\\categorical_labels_gender.npy')\n",
    "\n",
    "X = np.load(r'..\\data\\04_data_array\\data_32_32_rgb.npy')\n",
    "\n",
    "size = len(X)\n",
    "print(\"La longuitud del los datos son de:\", size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un 30% de las iamgenes como test y el 70% como train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion train y test set\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = (X[:15008],categorical_labels_gender[:15008]) , (X[15008:] , categorical_labels_gender[15008:])\n",
    "(x_valid , y_valid) = (x_test[:7000], y_test[:7000])\n",
    "(x_test, y_test) = (x_test[7000:], y_test[7000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imagenes en el train: 15008\n",
      "Número de imagenes en el test: 2091\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de imagenes en el train:\",x_train.shape[0])\n",
    "print(\"Número de imagenes en el test:\",x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "## SGD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 128)       1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,165,250\n",
      "Trainable params: 2,165,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "151/151 [==============================] - 180s 1s/step - loss: 0.6958 - accuracy: 0.5221 - auc: 0.5291 - categorical_accuracy: 0.5221 - val_loss: 0.6881 - val_accuracy: 0.5497 - val_auc: 0.5707 - val_categorical_accuracy: 0.5497\n",
      "Epoch 2/25\n",
      "151/151 [==============================] - 180s 1s/step - loss: 0.6869 - accuracy: 0.5491 - auc: 0.5649 - categorical_accuracy: 0.5491 - val_loss: 0.6849 - val_accuracy: 0.5286 - val_auc: 0.5804 - val_categorical_accuracy: 0.5286\n",
      "Epoch 3/25\n",
      "151/151 [==============================] - 179s 1s/step - loss: 0.6815 - accuracy: 0.5646 - auc: 0.5874 - categorical_accuracy: 0.5646 - val_loss: 0.6772 - val_accuracy: 0.5747 - val_auc: 0.6123 - val_categorical_accuracy: 0.5747\n",
      "Epoch 4/25\n",
      "151/151 [==============================] - 171s 1s/step - loss: 0.6752 - accuracy: 0.5758 - auc: 0.6085 - categorical_accuracy: 0.5758 - val_loss: 0.6695 - val_accuracy: 0.6077 - val_auc: 0.6442 - val_categorical_accuracy: 0.6077\n",
      "Epoch 5/25\n",
      "151/151 [==============================] - 170s 1s/step - loss: 0.6702 - accuracy: 0.5917 - auc: 0.6233 - categorical_accuracy: 0.5917 - val_loss: 0.6679 - val_accuracy: 0.5859 - val_auc: 0.6315 - val_categorical_accuracy: 0.5859\n",
      "Epoch 6/25\n",
      "151/151 [==============================] - 179s 1s/step - loss: 0.6636 - accuracy: 0.6051 - auc: 0.6428 - categorical_accuracy: 0.6051 - val_loss: 0.6608 - val_accuracy: 0.6026 - val_auc: 0.6508 - val_categorical_accuracy: 0.6026\n",
      "Epoch 7/25\n",
      "151/151 [==============================] - 176s 1s/step - loss: 0.6584 - accuracy: 0.6114 - auc: 0.6519 - categorical_accuracy: 0.6114 - val_loss: 0.6758 - val_accuracy: 0.5609 - val_auc: 0.6051 - val_categorical_accuracy: 0.5609\n",
      "Epoch 8/25\n",
      "151/151 [==============================] - 155s 1s/step - loss: 0.6520 - accuracy: 0.6193 - auc: 0.6646 - categorical_accuracy: 0.6193 - val_loss: 0.6492 - val_accuracy: 0.6256 - val_auc: 0.6757 - val_categorical_accuracy: 0.6256\n",
      "Epoch 9/25\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 0.6491 - accuracy: 0.6281 - auc: 0.6715 - categorical_accuracy: 0.6281 - val_loss: 0.6730 - val_accuracy: 0.5759 - val_auc: 0.6310 - val_categorical_accuracy: 0.5759\n",
      "Epoch 10/25\n",
      "151/151 [==============================] - 117s 776ms/step - loss: 0.6447 - accuracy: 0.6309 - auc: 0.6788 - categorical_accuracy: 0.6309 - val_loss: 0.6440 - val_accuracy: 0.6291 - val_auc: 0.6824 - val_categorical_accuracy: 0.6291\n",
      "Epoch 11/25\n",
      "151/151 [==============================] - 117s 772ms/step - loss: 0.6415 - accuracy: 0.6367 - auc: 0.6847 - categorical_accuracy: 0.6367 - val_loss: 0.6395 - val_accuracy: 0.6481 - val_auc: 0.6968 - val_categorical_accuracy: 0.6481\n",
      "Epoch 12/25\n",
      "151/151 [==============================] - 127s 838ms/step - loss: 0.6380 - accuracy: 0.6392 - auc: 0.6898 - categorical_accuracy: 0.6392 - val_loss: 0.6400 - val_accuracy: 0.6317 - val_auc: 0.6832 - val_categorical_accuracy: 0.6317\n",
      "Epoch 13/25\n",
      "151/151 [==============================] - 132s 877ms/step - loss: 0.6372 - accuracy: 0.6429 - auc: 0.6912 - categorical_accuracy: 0.6429 - val_loss: 0.6328 - val_accuracy: 0.6496 - val_auc: 0.7023 - val_categorical_accuracy: 0.6496\n",
      "Epoch 14/25\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 0.6337 - accuracy: 0.6451 - auc: 0.6971 - categorical_accuracy: 0.6451 - val_loss: 0.6364 - val_accuracy: 0.6520 - val_auc: 0.6994 - val_categorical_accuracy: 0.6520\n",
      "Epoch 15/25\n",
      "151/151 [==============================] - 115s 764ms/step - loss: 0.6317 - accuracy: 0.6465 - auc: 0.6996 - categorical_accuracy: 0.6465 - val_loss: 0.6239 - val_accuracy: 0.6586 - val_auc: 0.7220 - val_categorical_accuracy: 0.6586\n",
      "Epoch 16/25\n",
      "151/151 [==============================] - 134s 889ms/step - loss: 0.6268 - accuracy: 0.6528 - auc: 0.7072 - categorical_accuracy: 0.6528 - val_loss: 0.6437 - val_accuracy: 0.6171 - val_auc: 0.6757 - val_categorical_accuracy: 0.6171\n",
      "Epoch 17/25\n",
      "151/151 [==============================] - 117s 777ms/step - loss: 0.6251 - accuracy: 0.6528 - auc: 0.7095 - categorical_accuracy: 0.6528 - val_loss: 0.6208 - val_accuracy: 0.6670 - val_auc: 0.7284 - val_categorical_accuracy: 0.6670\n",
      "Epoch 18/25\n",
      "151/151 [==============================] - 122s 807ms/step - loss: 0.6214 - accuracy: 0.6577 - auc: 0.7144 - categorical_accuracy: 0.6577 - val_loss: 0.6161 - val_accuracy: 0.6716 - val_auc: 0.7334 - val_categorical_accuracy: 0.6716\n",
      "Epoch 19/25\n",
      "151/151 [==============================] - 133s 883ms/step - loss: 0.6204 - accuracy: 0.6611 - auc: 0.7160 - categorical_accuracy: 0.6611 - val_loss: 0.6167 - val_accuracy: 0.6627 - val_auc: 0.7212 - val_categorical_accuracy: 0.6627\n",
      "Epoch 20/25\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.6186 - accuracy: 0.6606 - auc: 0.7185 - categorical_accuracy: 0.6606 - val_loss: 0.6125 - val_accuracy: 0.6661 - val_auc: 0.7289 - val_categorical_accuracy: 0.6661\n",
      "Epoch 21/25\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 0.6130 - accuracy: 0.6684 - auc: 0.7260 - categorical_accuracy: 0.6684 - val_loss: 0.6183 - val_accuracy: 0.6563 - val_auc: 0.7132 - val_categorical_accuracy: 0.6563\n",
      "Epoch 22/25\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.6106 - accuracy: 0.6700 - auc: 0.7286 - categorical_accuracy: 0.6700 - val_loss: 0.6044 - val_accuracy: 0.6759 - val_auc: 0.7401 - val_categorical_accuracy: 0.6759\n",
      "Epoch 23/25\n",
      "151/151 [==============================] - 136s 900ms/step - loss: 0.6097 - accuracy: 0.6702 - auc: 0.7297 - categorical_accuracy: 0.6702 - val_loss: 0.6082 - val_accuracy: 0.6634 - val_auc: 0.7296 - val_categorical_accuracy: 0.6634\n",
      "Epoch 24/25\n",
      "151/151 [==============================] - 122s 806ms/step - loss: 0.6066 - accuracy: 0.6722 - auc: 0.7336 - categorical_accuracy: 0.6722 - val_loss: 0.6215 - val_accuracy: 0.6459 - val_auc: 0.7069 - val_categorical_accuracy: 0.6459\n",
      "Epoch 25/25\n",
      "151/151 [==============================] - 127s 844ms/step - loss: 0.6050 - accuracy: 0.6677 - auc: 0.7341 - categorical_accuracy: 0.6677 - val_loss: 0.5984 - val_accuracy: 0.6834 - val_auc: 0.7501 - val_categorical_accuracy: 0.6834\n"
     ]
    }
   ],
   "source": [
    "model_sgd_gender = tf.keras.Sequential()\n",
    "\n",
    "# Primera capa de la CNN. El imput debe ajsutarse al tamañ ode la imagen (32 x 32 y 3 de los colores)\n",
    "\n",
    "model_sgd_gender.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(32,32,3))) \n",
    "model_sgd_gender.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model_sgd_gender.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model_sgd_gender.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu')) \n",
    "model_sgd_gender.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "# drop out= eliminamos el 30% de las imagenes para evitar overfitting\n",
    "model_sgd_gender.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model_sgd_gender.add(tf.keras.layers.Flatten())\n",
    "model_sgd_gender.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "# drop out= eliminamos la mitad de las imagenes para evitar overfitting\n",
    "model_sgd_gender.add(tf.keras.layers.Dropout(0.5)) \n",
    "\n",
    "\n",
    "# nuestro data set tiene 5 opciones de raza luego la última capa = 5\n",
    "\n",
    "model_sgd_gender.add(tf.keras.layers.Dense(2, activation='softmax')) \n",
    "\n",
    "\n",
    "# Resumen del modelo\n",
    "model_sgd_gender.summary()\n",
    "\n",
    "model_sgd_gender.compile(loss='binary_crossentropy',\n",
    "             optimizer=SGD(),\n",
    "             metrics=['accuracy','AUC','categorical_accuracy'])\n",
    "\n",
    "h_model_sgd_gender = model_sgd_gender.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=100,\n",
    "         epochs=25,\n",
    "         validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Apuntes\\\\Desktop\\\\labUTKfaces\\\\models\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_sgd_gender_history = h_model_sgd_gender.history\n",
    "json.dump(model_sgd_gender_history, open(\"../models/model_sgd_gender_history.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_test_genero.npy', x_test)\n",
    "np.save('y_test_genero.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Apuntes\\anaconda\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Apuntes\\anaconda\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../models/model_sgd_gender_rgb.h\\assets\n"
     ]
    }
   ],
   "source": [
    "model_sgd_gender.save('../models/model_sgd_gender_rgb.h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,165,250\n",
      "Trainable params: 2,165,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.6842 - accuracy: 0.5748 - auc: 0.6065 - categorical_accuracy: 0.5748 - val_loss: 0.6184 - val_accuracy: 0.6537 - val_auc: 0.7159 - val_categorical_accuracy: 0.6537\n",
      "Epoch 2/25\n",
      "151/151 [==============================] - 192s 1s/step - loss: 0.6061 - accuracy: 0.6706 - auc: 0.7318 - categorical_accuracy: 0.6706 - val_loss: 0.5769 - val_accuracy: 0.6890 - val_auc: 0.7616 - val_categorical_accuracy: 0.6890\n",
      "Epoch 3/25\n",
      "151/151 [==============================] - 191s 1s/step - loss: 0.5738 - accuracy: 0.6933 - auc: 0.7666 - categorical_accuracy: 0.6933 - val_loss: 0.5520 - val_accuracy: 0.7090 - val_auc: 0.7865 - val_categorical_accuracy: 0.7090\n",
      "Epoch 4/25\n",
      "151/151 [==============================] - 176s 1s/step - loss: 0.5472 - accuracy: 0.7132 - auc: 0.7930 - categorical_accuracy: 0.7132 - val_loss: 0.5229 - val_accuracy: 0.7331 - val_auc: 0.8144 - val_categorical_accuracy: 0.7331\n",
      "Epoch 5/25\n",
      "151/151 [==============================] - 172s 1s/step - loss: 0.5270 - accuracy: 0.7341 - auc: 0.8122 - categorical_accuracy: 0.7341 - val_loss: 0.5118 - val_accuracy: 0.7381 - val_auc: 0.8240 - val_categorical_accuracy: 0.7381\n",
      "Epoch 6/25\n",
      "151/151 [==============================] - 177s 1s/step - loss: 0.5121 - accuracy: 0.7383 - auc: 0.8230 - categorical_accuracy: 0.7383 - val_loss: 0.5009 - val_accuracy: 0.7489 - val_auc: 0.8320 - val_categorical_accuracy: 0.7489\n",
      "Epoch 7/25\n",
      "151/151 [==============================] - 175s 1s/step - loss: 0.4941 - accuracy: 0.7514 - auc: 0.8378 - categorical_accuracy: 0.7514 - val_loss: 0.4974 - val_accuracy: 0.7473 - val_auc: 0.8345 - val_categorical_accuracy: 0.7473\n",
      "Epoch 8/25\n",
      "151/151 [==============================] - 174s 1s/step - loss: 0.4846 - accuracy: 0.7575 - auc: 0.8449 - categorical_accuracy: 0.7575 - val_loss: 0.4881 - val_accuracy: 0.7520 - val_auc: 0.8413 - val_categorical_accuracy: 0.7520\n",
      "Epoch 9/25\n",
      "151/151 [==============================] - 173s 1s/step - loss: 0.4750 - accuracy: 0.7683 - auc: 0.8522 - categorical_accuracy: 0.7683 - val_loss: 0.4822 - val_accuracy: 0.7599 - val_auc: 0.8463 - val_categorical_accuracy: 0.7599\n",
      "Epoch 10/25\n",
      "151/151 [==============================] - 176s 1s/step - loss: 0.4585 - accuracy: 0.7760 - auc: 0.8641 - categorical_accuracy: 0.7760 - val_loss: 0.4868 - val_accuracy: 0.7530 - val_auc: 0.8415 - val_categorical_accuracy: 0.7530\n",
      "Epoch 11/25\n",
      "151/151 [==============================] - 173s 1s/step - loss: 0.4502 - accuracy: 0.7836 - auc: 0.8693 - categorical_accuracy: 0.7836 - val_loss: 0.4751 - val_accuracy: 0.7649 - val_auc: 0.8513 - val_categorical_accuracy: 0.7649\n",
      "Epoch 12/25\n",
      "151/151 [==============================] - 177s 1s/step - loss: 0.4390 - accuracy: 0.7881 - auc: 0.8761 - categorical_accuracy: 0.7881 - val_loss: 0.4759 - val_accuracy: 0.7651 - val_auc: 0.8521 - val_categorical_accuracy: 0.7651\n",
      "Epoch 13/25\n",
      "151/151 [==============================] - 177s 1s/step - loss: 0.4227 - accuracy: 0.8017 - auc: 0.8870 - categorical_accuracy: 0.8017 - val_loss: 0.4860 - val_accuracy: 0.7571 - val_auc: 0.8461 - val_categorical_accuracy: 0.7571\n",
      "Epoch 14/25\n",
      "151/151 [==============================] - 168s 1s/step - loss: 0.4130 - accuracy: 0.8072 - auc: 0.8922 - categorical_accuracy: 0.8072 - val_loss: 0.4736 - val_accuracy: 0.7686 - val_auc: 0.8525 - val_categorical_accuracy: 0.7686\n",
      "Epoch 15/25\n",
      "151/151 [==============================] - 171s 1s/step - loss: 0.4103 - accuracy: 0.8090 - auc: 0.8942 - categorical_accuracy: 0.8090 - val_loss: 0.4728 - val_accuracy: 0.7694 - val_auc: 0.8548 - val_categorical_accuracy: 0.7694\n",
      "Epoch 16/25\n",
      "151/151 [==============================] - 173s 1s/step - loss: 0.3888 - accuracy: 0.8198 - auc: 0.9050 - categorical_accuracy: 0.8198 - val_loss: 0.4699 - val_accuracy: 0.7686 - val_auc: 0.8578 - val_categorical_accuracy: 0.7686\n",
      "Epoch 17/25\n",
      "151/151 [==============================] - 177s 1s/step - loss: 0.3739 - accuracy: 0.8228 - auc: 0.9120 - categorical_accuracy: 0.8228 - val_loss: 0.4854 - val_accuracy: 0.7641 - val_auc: 0.8476 - val_categorical_accuracy: 0.7641\n",
      "Epoch 18/25\n",
      "151/151 [==============================] - 173s 1s/step - loss: 0.3676 - accuracy: 0.8313 - auc: 0.9163 - categorical_accuracy: 0.8313 - val_loss: 0.4931 - val_accuracy: 0.7611 - val_auc: 0.8518 - val_categorical_accuracy: 0.7611\n",
      "Epoch 19/25\n",
      "151/151 [==============================] - 166s 1s/step - loss: 0.3506 - accuracy: 0.8422 - auc: 0.9242 - categorical_accuracy: 0.8422 - val_loss: 0.4754 - val_accuracy: 0.7736 - val_auc: 0.8586 - val_categorical_accuracy: 0.7736\n",
      "Epoch 20/25\n",
      "151/151 [==============================] - 179s 1s/step - loss: 0.3423 - accuracy: 0.8444 - auc: 0.9278 - categorical_accuracy: 0.8444 - val_loss: 0.4913 - val_accuracy: 0.7631 - val_auc: 0.8534 - val_categorical_accuracy: 0.7631\n",
      "Epoch 21/25\n",
      "151/151 [==============================] - 166s 1s/step - loss: 0.3291 - accuracy: 0.8563 - auc: 0.9345 - categorical_accuracy: 0.8563 - val_loss: 0.4955 - val_accuracy: 0.7677 - val_auc: 0.8527 - val_categorical_accuracy: 0.7677\n",
      "Epoch 22/25\n",
      "151/151 [==============================] - 175s 1s/step - loss: 0.3173 - accuracy: 0.8589 - auc: 0.9383 - categorical_accuracy: 0.8589 - val_loss: 0.4934 - val_accuracy: 0.7701 - val_auc: 0.8560 - val_categorical_accuracy: 0.7701\n",
      "Epoch 23/25\n",
      "151/151 [==============================] - 176s 1s/step - loss: 0.3000 - accuracy: 0.8679 - auc: 0.9454 - categorical_accuracy: 0.8679 - val_loss: 0.5134 - val_accuracy: 0.7701 - val_auc: 0.8543 - val_categorical_accuracy: 0.7701\n",
      "Epoch 24/25\n",
      "151/151 [==============================] - 175s 1s/step - loss: 0.2977 - accuracy: 0.8729 - auc: 0.9467 - categorical_accuracy: 0.8729 - val_loss: 0.4893 - val_accuracy: 0.7691 - val_auc: 0.8574 - val_categorical_accuracy: 0.7691\n",
      "Epoch 25/25\n",
      "151/151 [==============================] - 167s 1s/step - loss: 0.2860 - accuracy: 0.8742 - auc: 0.9503 - categorical_accuracy: 0.8742 - val_loss: 0.5076 - val_accuracy: 0.7699 - val_auc: 0.8561 - val_categorical_accuracy: 0.7699\n"
     ]
    }
   ],
   "source": [
    "model_adam_gender = tf.keras.Sequential()\n",
    "\n",
    "# Primera capa de la CNN. El imput debe ajsutarse al tamañ ode la imagen (32 x 32 y 3 de los colores)\n",
    "model_adam_gender.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(32,32,3))) \n",
    "model_adam_gender.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model_adam_gender.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model_adam_gender.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu')) \n",
    "model_adam_gender.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model_adam_gender.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model_adam_gender.add(tf.keras.layers.Flatten())\n",
    "model_adam_gender.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model_adam_gender.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model_adam_gender.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model_adam_gender.summary()\n",
    "\n",
    "model_adam_gender.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy','AUC','categorical_accuracy'])\n",
    "\n",
    "h_model_adam_gender = model_adam_gender.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=100,\n",
    "         epochs=25,\n",
    "         validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_adam_gender_history = h_model_adam_gender.history\n",
    "json.dump(model_adam_gender_history, open(\"../models/model_adam_gender_history.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/model_adam_gender_rgb.h\\assets\n"
     ]
    }
   ],
   "source": [
    "model_adam_gender.save('../models/model_adam_gender_rgb.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
