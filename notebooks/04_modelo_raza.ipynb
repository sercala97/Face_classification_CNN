{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nombre: **04_modelo_raza**\n",
    "#### Autores: _Sergio Cañón  Laiz y Ignacio Ruiz de Zuazu Echavarría_\n",
    "#### CUNEF\n",
    "#### 15/01/2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerías \n",
    "\n",
    "import numpy as np  # álgebra lineal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import glob  # pathnames\n",
    "import cv2  # lectura y procesamiento de las imagenes\n",
    "\n",
    "import matplotlib.pyplot as plt  # gráficos\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import os  # lectura de directorios\n",
    "\n",
    "import keras \n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import time #tiempos de ejecución\n",
    "from random import shuffle\n",
    "from keras.preprocessing import image\n",
    "import imageio\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_labels_race = np.load(r'..\\data\\04_data_array\\categorical_labels_race.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(r'..\\data\\04_data_array\\data_32_32_rgb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longuitud del los datos son de: 24099\n"
     ]
    }
   ],
   "source": [
    "size = len(X)\n",
    "print(\"La longuitud del los datos son de:\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un 30% de las iamgenes como test y el 70% como train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion train y test set\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = (X[:15008],categorical_labels_race[:15008]) , (X[15008:] , categorical_labels_race[15008:])\n",
    "(x_valid , y_valid) = (x_test[:7000], y_test[:7000])\n",
    "(x_test, y_test) = (x_test[7000:], y_test[7000:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imagenes en el train: 15008\n",
      "Número de imagenes en el test: 2091\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de imagenes en el train:\",x_train.shape[0])\n",
    "print(\"Número de imagenes en el test:\",x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 128)       1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,166,021\n",
      "Trainable params: 2,166,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "151/151 [==============================] - 63s 418ms/step - loss: 1.4727 - accuracy: 0.4156 - auc: 0.6800 - categorical_accuracy: 0.4156 - val_loss: 1.4497 - val_accuracy: 0.4277 - val_auc: 0.7145 - val_categorical_accuracy: 0.4277\n",
      "Epoch 2/25\n",
      "151/151 [==============================] - 69s 455ms/step - loss: 1.4481 - accuracy: 0.4248 - auc: 0.6959 - categorical_accuracy: 0.4248 - val_loss: 1.4513 - val_accuracy: 0.4289 - val_auc: 0.7135 - val_categorical_accuracy: 0.4289\n",
      "Epoch 3/25\n",
      "151/151 [==============================] - 72s 479ms/step - loss: 1.4366 - accuracy: 0.4230 - auc: 0.7034 - categorical_accuracy: 0.4230 - val_loss: 1.4205 - val_accuracy: 0.4293 - val_auc: 0.7212 - val_categorical_accuracy: 0.4293\n",
      "Epoch 4/25\n",
      "151/151 [==============================] - 67s 446ms/step - loss: 1.4302 - accuracy: 0.4270 - auc: 0.7074 - categorical_accuracy: 0.4270 - val_loss: 1.4233 - val_accuracy: 0.4320 - val_auc: 0.7285 - val_categorical_accuracy: 0.4320\n",
      "Epoch 5/25\n",
      "151/151 [==============================] - 67s 442ms/step - loss: 1.4256 - accuracy: 0.4298 - auc: 0.7113 - categorical_accuracy: 0.4298 - val_loss: 1.4209 - val_accuracy: 0.4331 - val_auc: 0.7301 - val_categorical_accuracy: 0.4331\n",
      "Epoch 6/25\n",
      "151/151 [==============================] - 75s 497ms/step - loss: 1.4192 - accuracy: 0.4325 - auc: 0.7154 - categorical_accuracy: 0.4325 - val_loss: 1.4018 - val_accuracy: 0.4353 - val_auc: 0.7327 - val_categorical_accuracy: 0.4353\n",
      "Epoch 7/25\n",
      "151/151 [==============================] - 69s 454ms/step - loss: 1.4129 - accuracy: 0.4360 - auc: 0.7193 - categorical_accuracy: 0.4360 - val_loss: 1.4194 - val_accuracy: 0.4379 - val_auc: 0.7371 - val_categorical_accuracy: 0.4379\n",
      "Epoch 8/25\n",
      "151/151 [==============================] - 62s 412ms/step - loss: 1.4107 - accuracy: 0.4360 - auc: 0.7212 - categorical_accuracy: 0.4360 - val_loss: 1.3905 - val_accuracy: 0.4344 - val_auc: 0.7380 - val_categorical_accuracy: 0.4344\n",
      "Epoch 9/25\n",
      "151/151 [==============================] - 59s 393ms/step - loss: 1.3993 - accuracy: 0.4386 - auc: 0.7270 - categorical_accuracy: 0.4386 - val_loss: 1.3904 - val_accuracy: 0.4427 - val_auc: 0.7418 - val_categorical_accuracy: 0.4427\n",
      "Epoch 10/25\n",
      "151/151 [==============================] - 67s 442ms/step - loss: 1.3969 - accuracy: 0.4408 - auc: 0.7287 - categorical_accuracy: 0.4408 - val_loss: 1.3784 - val_accuracy: 0.4423 - val_auc: 0.7426 - val_categorical_accuracy: 0.4423\n",
      "Epoch 11/25\n",
      "151/151 [==============================] - 72s 479ms/step - loss: 1.3930 - accuracy: 0.4441 - auc: 0.7306 - categorical_accuracy: 0.4441 - val_loss: 1.3755 - val_accuracy: 0.4431 - val_auc: 0.7427 - val_categorical_accuracy: 0.4431\n",
      "Epoch 12/25\n",
      "151/151 [==============================] - 71s 470ms/step - loss: 1.3873 - accuracy: 0.4460 - auc: 0.7335 - categorical_accuracy: 0.4460 - val_loss: 1.3865 - val_accuracy: 0.4551 - val_auc: 0.7396 - val_categorical_accuracy: 0.4551\n",
      "Epoch 13/25\n",
      "151/151 [==============================] - 64s 425ms/step - loss: 1.3841 - accuracy: 0.4460 - auc: 0.7347 - categorical_accuracy: 0.4460 - val_loss: 1.3828 - val_accuracy: 0.4573 - val_auc: 0.7429 - val_categorical_accuracy: 0.4573\n",
      "Epoch 14/25\n",
      "151/151 [==============================] - 112s 740ms/step - loss: 1.3785 - accuracy: 0.4457 - auc: 0.7376 - categorical_accuracy: 0.4457 - val_loss: 1.3741 - val_accuracy: 0.4550 - val_auc: 0.7480 - val_categorical_accuracy: 0.4550\n",
      "Epoch 15/25\n",
      "151/151 [==============================] - 126s 838ms/step - loss: 1.3777 - accuracy: 0.4534 - auc: 0.7377 - categorical_accuracy: 0.4534 - val_loss: 1.3667 - val_accuracy: 0.4494 - val_auc: 0.7495 - val_categorical_accuracy: 0.4494\n",
      "Epoch 16/25\n",
      "151/151 [==============================] - 126s 838ms/step - loss: 1.3734 - accuracy: 0.4525 - auc: 0.7399 - categorical_accuracy: 0.4525 - val_loss: 1.3692 - val_accuracy: 0.4487 - val_auc: 0.7493 - val_categorical_accuracy: 0.4487\n",
      "Epoch 17/25\n",
      "151/151 [==============================] - 128s 846ms/step - loss: 1.3713 - accuracy: 0.4508 - auc: 0.7413 - categorical_accuracy: 0.4508 - val_loss: 1.3612 - val_accuracy: 0.4441 - val_auc: 0.7484 - val_categorical_accuracy: 0.4441\n",
      "Epoch 18/25\n",
      "151/151 [==============================] - 159s 1s/step - loss: 1.3703 - accuracy: 0.4530 - auc: 0.7413 - categorical_accuracy: 0.4530 - val_loss: 1.3663 - val_accuracy: 0.4567 - val_auc: 0.7494 - val_categorical_accuracy: 0.4567\n",
      "Epoch 19/25\n",
      "151/151 [==============================] - 179s 1s/step - loss: 1.3670 - accuracy: 0.4521 - auc: 0.7430 - categorical_accuracy: 0.4521 - val_loss: 1.3539 - val_accuracy: 0.4517 - val_auc: 0.7527 - val_categorical_accuracy: 0.4517\n",
      "Epoch 20/25\n",
      "151/151 [==============================] - 183s 1s/step - loss: 1.3659 - accuracy: 0.4509 - auc: 0.7434 - categorical_accuracy: 0.4509 - val_loss: 1.3521 - val_accuracy: 0.4527 - val_auc: 0.7533 - val_categorical_accuracy: 0.4527\n",
      "Epoch 21/25\n",
      "151/151 [==============================] - 163s 1s/step - loss: 1.3632 - accuracy: 0.4530 - auc: 0.7450 - categorical_accuracy: 0.4530 - val_loss: 1.3551 - val_accuracy: 0.4587 - val_auc: 0.7508 - val_categorical_accuracy: 0.4587\n",
      "Epoch 22/25\n",
      "151/151 [==============================] - 176s 1s/step - loss: 1.3632 - accuracy: 0.4558 - auc: 0.7449 - categorical_accuracy: 0.4558 - val_loss: 1.3551 - val_accuracy: 0.4506 - val_auc: 0.7534 - val_categorical_accuracy: 0.4506\n",
      "Epoch 23/25\n",
      "151/151 [==============================] - 179s 1s/step - loss: 1.3586 - accuracy: 0.4570 - auc: 0.7469 - categorical_accuracy: 0.4570 - val_loss: 1.3586 - val_accuracy: 0.4561 - val_auc: 0.7557 - val_categorical_accuracy: 0.4561\n",
      "Epoch 24/25\n",
      "151/151 [==============================] - 170s 1s/step - loss: 1.3589 - accuracy: 0.4552 - auc: 0.7468 - categorical_accuracy: 0.4552 - val_loss: 1.3601 - val_accuracy: 0.4607 - val_auc: 0.7516 - val_categorical_accuracy: 0.4607\n",
      "Epoch 25/25\n",
      "151/151 [==============================] - 177s 1s/step - loss: 1.3554 - accuracy: 0.4573 - auc: 0.7486 - categorical_accuracy: 0.4573 - val_loss: 1.3438 - val_accuracy: 0.4513 - val_auc: 0.7566 - val_categorical_accuracy: 0.4513\n"
     ]
    }
   ],
   "source": [
    "model_sgd = tf.keras.Sequential()\n",
    "\n",
    "# Primera capa de la CNN. El imput debe ajsutarse al tamañ ode la imagen (32 x 32 y 3 de los colores)\n",
    "\n",
    "model_sgd.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(32,32,3))) \n",
    "model_sgd.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model_sgd.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model_sgd.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu')) \n",
    "model_sgd.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "# drop out= eliminamos el 30% de las imagenes para evitar overfitting\n",
    "model_sgd.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model_sgd.add(tf.keras.layers.Flatten())\n",
    "model_sgd.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "# drop out= eliminamos la mitad de las imagenes para evitar overfitting\n",
    "model_sgd.add(tf.keras.layers.Dropout(0.5)) \n",
    "\n",
    "\n",
    "# nuestro data set tiene 5 opciones de raza luego la última capa = 5\n",
    "\n",
    "model_sgd.add(tf.keras.layers.Dense(5, activation='softmax')) \n",
    "\n",
    "\n",
    "# Resumen del modelo\n",
    "model_sgd.summary()\n",
    "\n",
    "model_sgd.compile(loss='categorical_crossentropy',\n",
    "             optimizer=SGD(),\n",
    "             metrics=['accuracy','AUC','categorical_accuracy'])\n",
    "\n",
    "h_sgd = model_sgd.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=100,\n",
    "         epochs=25,\n",
    "         validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd_race_rgb_history = h_sgd.history\n",
    "json.dump(model_sgd_race_rgb_history, open(\"../models/model_sgd_race_rgb_history.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/model_sgd_race_rgb.h\\assets\n"
     ]
    }
   ],
   "source": [
    "model_sgd_gender.save('../models/model_sgd_race_rgb.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_test_raza.npy', x_test)\n",
    "np.save('y_test_raza.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 128)       1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,166,021\n",
      "Trainable params: 2,166,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "151/151 [==============================] - 185s 1s/step - loss: 1.4453 - accuracy: 0.4242 - auc: 0.7023 - categorical_accuracy: 0.4242 - val_loss: 1.3676 - val_accuracy: 0.4571 - val_auc: 0.7529 - val_categorical_accuracy: 0.4571\n",
      "Epoch 2/25\n",
      "151/151 [==============================] - 200s 1s/step - loss: 1.3598 - accuracy: 0.4561 - auc: 0.7465 - categorical_accuracy: 0.4561 - val_loss: 1.3192 - val_accuracy: 0.4833 - val_auc: 0.7762 - val_categorical_accuracy: 0.4833\n",
      "Epoch 3/25\n",
      "151/151 [==============================] - 181s 1s/step - loss: 1.3183 - accuracy: 0.4711 - auc: 0.7659 - categorical_accuracy: 0.4711 - val_loss: 1.2957 - val_accuracy: 0.4777 - val_auc: 0.7732 - val_categorical_accuracy: 0.4777\n",
      "Epoch 4/25\n",
      "151/151 [==============================] - 173s 1s/step - loss: 1.2868 - accuracy: 0.4875 - auc: 0.7783 - categorical_accuracy: 0.4875 - val_loss: 1.2489 - val_accuracy: 0.5024 - val_auc: 0.7963 - val_categorical_accuracy: 0.5024\n",
      "Epoch 5/25\n",
      "151/151 [==============================] - 177s 1s/step - loss: 1.2553 - accuracy: 0.5029 - auc: 0.7907 - categorical_accuracy: 0.5029 - val_loss: 1.2170 - val_accuracy: 0.5223 - val_auc: 0.8072 - val_categorical_accuracy: 0.5223\n",
      "Epoch 6/25\n",
      "151/151 [==============================] - 172s 1s/step - loss: 1.2298 - accuracy: 0.5157 - auc: 0.8008 - categorical_accuracy: 0.5157 - val_loss: 1.2241 - val_accuracy: 0.5143 - val_auc: 0.8034 - val_categorical_accuracy: 0.5143\n",
      "Epoch 7/25\n",
      "151/151 [==============================] - 170s 1s/step - loss: 1.2191 - accuracy: 0.5207 - auc: 0.8051 - categorical_accuracy: 0.5207 - val_loss: 1.1897 - val_accuracy: 0.5350 - val_auc: 0.8141 - val_categorical_accuracy: 0.5350\n",
      "Epoch 8/25\n",
      "151/151 [==============================] - 174s 1s/step - loss: 1.1798 - accuracy: 0.5403 - auc: 0.8185 - categorical_accuracy: 0.5403 - val_loss: 1.1952 - val_accuracy: 0.5379 - val_auc: 0.8128 - val_categorical_accuracy: 0.5379\n",
      "Epoch 9/25\n",
      "151/151 [==============================] - 175s 1s/step - loss: 1.1642 - accuracy: 0.5445 - auc: 0.8243 - categorical_accuracy: 0.5445 - val_loss: 1.1938 - val_accuracy: 0.5391 - val_auc: 0.8151 - val_categorical_accuracy: 0.5391\n",
      "Epoch 10/25\n",
      "151/151 [==============================] - 175s 1s/step - loss: 1.1360 - accuracy: 0.5584 - auc: 0.8331 - categorical_accuracy: 0.5584 - val_loss: 1.1699 - val_accuracy: 0.5463 - val_auc: 0.8223 - val_categorical_accuracy: 0.5463\n",
      "Epoch 11/25\n",
      "151/151 [==============================] - 173s 1s/step - loss: 1.1108 - accuracy: 0.5649 - auc: 0.8408 - categorical_accuracy: 0.5649 - val_loss: 1.1662 - val_accuracy: 0.5423 - val_auc: 0.8217 - val_categorical_accuracy: 0.5423\n",
      "Epoch 12/25\n",
      "151/151 [==============================] - 174s 1s/step - loss: 1.0893 - accuracy: 0.5783 - auc: 0.8474 - categorical_accuracy: 0.5783 - val_loss: 1.1494 - val_accuracy: 0.5586 - val_auc: 0.8273 - val_categorical_accuracy: 0.5586\n",
      "Epoch 13/25\n",
      "151/151 [==============================] - 177s 1s/step - loss: 1.0642 - accuracy: 0.5902 - auc: 0.8550 - categorical_accuracy: 0.5902 - val_loss: 1.1634 - val_accuracy: 0.5480 - val_auc: 0.8232 - val_categorical_accuracy: 0.5480\n",
      "Epoch 14/25\n",
      "151/151 [==============================] - 175s 1s/step - loss: 1.0386 - accuracy: 0.6005 - auc: 0.8626 - categorical_accuracy: 0.6005 - val_loss: 1.1859 - val_accuracy: 0.5419 - val_auc: 0.8165 - val_categorical_accuracy: 0.5419\n",
      "Epoch 15/25\n",
      "151/151 [==============================] - 168s 1s/step - loss: 1.0162 - accuracy: 0.6079 - auc: 0.8689 - categorical_accuracy: 0.6079 - val_loss: 1.1447 - val_accuracy: 0.5630 - val_auc: 0.8300 - val_categorical_accuracy: 0.5630\n",
      "Epoch 16/25\n",
      "151/151 [==============================] - 173s 1s/step - loss: 0.9932 - accuracy: 0.6211 - auc: 0.8748 - categorical_accuracy: 0.6211 - val_loss: 1.1433 - val_accuracy: 0.5636 - val_auc: 0.8308 - val_categorical_accuracy: 0.5636\n",
      "Epoch 17/25\n",
      "151/151 [==============================] - 176s 1s/step - loss: 0.9659 - accuracy: 0.6295 - auc: 0.8820 - categorical_accuracy: 0.6295 - val_loss: 1.1893 - val_accuracy: 0.5450 - val_auc: 0.8172 - val_categorical_accuracy: 0.5450\n",
      "Epoch 18/25\n",
      "151/151 [==============================] - 174s 1s/step - loss: 0.9522 - accuracy: 0.6343 - auc: 0.8851 - categorical_accuracy: 0.6343 - val_loss: 1.1460 - val_accuracy: 0.5649 - val_auc: 0.8302 - val_categorical_accuracy: 0.5649\n",
      "Epoch 19/25\n",
      "151/151 [==============================] - 173s 1s/step - loss: 0.9152 - accuracy: 0.6437 - auc: 0.8941 - categorical_accuracy: 0.6437 - val_loss: 1.1517 - val_accuracy: 0.5677 - val_auc: 0.8309 - val_categorical_accuracy: 0.5677\n",
      "Epoch 20/25\n",
      "151/151 [==============================] - 172s 1s/step - loss: 0.8997 - accuracy: 0.6582 - auc: 0.8978 - categorical_accuracy: 0.6582 - val_loss: 1.1564 - val_accuracy: 0.5646 - val_auc: 0.8286 - val_categorical_accuracy: 0.5646\n",
      "Epoch 21/25\n",
      "151/151 [==============================] - 175s 1s/step - loss: 0.8736 - accuracy: 0.6636 - auc: 0.9038 - categorical_accuracy: 0.6636 - val_loss: 1.1696 - val_accuracy: 0.5603 - val_auc: 0.8285 - val_categorical_accuracy: 0.5603\n",
      "Epoch 22/25\n",
      "151/151 [==============================] - 167s 1s/step - loss: 0.8481 - accuracy: 0.6725 - auc: 0.9096 - categorical_accuracy: 0.6725 - val_loss: 1.2018 - val_accuracy: 0.5424 - val_auc: 0.8173 - val_categorical_accuracy: 0.5424\n",
      "Epoch 23/25\n",
      "151/151 [==============================] - 174s 1s/step - loss: 0.8210 - accuracy: 0.6861 - auc: 0.9151 - categorical_accuracy: 0.6861 - val_loss: 1.1880 - val_accuracy: 0.5600 - val_auc: 0.8275 - val_categorical_accuracy: 0.5600\n",
      "Epoch 24/25\n",
      "151/151 [==============================] - 175s 1s/step - loss: 0.7959 - accuracy: 0.6953 - auc: 0.9205 - categorical_accuracy: 0.6953 - val_loss: 1.2047 - val_accuracy: 0.5583 - val_auc: 0.8253 - val_categorical_accuracy: 0.5583\n",
      "Epoch 25/25\n",
      "151/151 [==============================] - 143s 949ms/step - loss: 0.7684 - accuracy: 0.7028 - auc: 0.9257 - categorical_accuracy: 0.7028 - val_loss: 1.1944 - val_accuracy: 0.5577 - val_auc: 0.8256 - val_categorical_accuracy: 0.5577\n"
     ]
    }
   ],
   "source": [
    "model_adam_race = tf.keras.Sequential()\n",
    "\n",
    "# Primera capa de la CNN. El imput debe ajsutarse al tamañ ode la imagen (32 x 32 y 3 de los colores)\n",
    "model_adam_race.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(32,32,3))) \n",
    "model_adam_race.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model_adam_race.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model_adam_race.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu')) \n",
    "model_adam_race.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model_adam_race.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model_adam_race.add(tf.keras.layers.Flatten())\n",
    "model_adam_race.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model_adam_race.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model_adam_race.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model_adam_race.summary()\n",
    "\n",
    "model_adam_race.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy','AUC','categorical_accuracy'])\n",
    "\n",
    "h_model_adam_race = model_adam_race.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=100,\n",
    "         epochs=25,\n",
    "         validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam_race_history = h_model_adam_race.history\n",
    "json.dump(model_adam_race_history, open(\"../models/model_adam_race_history.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/model_adam_race.h\\assets\n"
     ]
    }
   ],
   "source": [
    "model_adam_race.save('../models/model_adam_race.h')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
